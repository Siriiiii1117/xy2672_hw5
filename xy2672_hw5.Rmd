---
title: "xy2672_hw5"
output: html_document
date: "2025-11-14"
---

```{r setup, include=FALSE}
library(tidyverse)
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1 

Function of drawing `birthdays` out and return if there are duplicate `birthdays`

```{r}
repeat_birthday = function(n){
  birthdays = sample(1:365, n, replace = TRUE)
  
  repeated = length(unique(birthdays)) < n
  
  repeated
}
```

Run the function 10000 times for each group size between 2 and 50, then compute the probability that at least two people in the group will share a birthday

```{r}
sim_results_df = 
  expand_grid(
    n = 2:50,
    iter = 1:10000
  ) |> 
  mutate(
    results = map_lgl(n, repeat_birthday),
  ) |> 
  group_by(
    n
  ) |> 
  summarize(
    prob_same_bday = mean(results)
  ) 
```

Making a plot:

```{r}
sim_results_df |> 
  ggplot(aes(x = n, y = prob_same_bday)) + 
  geom_point() + 
  geom_line()
```

From the plot, we can see that as the number of people increases, the probability of having duplicate birthdays increases. The probability increases more rapidly from size 20 to 30, and reaches 0.5 when the sample size is 23. 


## HW2

Creating a function to generate the mu_hat and p-value:

```{r}
rnorm_sim = function(mu){
  sample = rnorm(30, mean = mu, sd = 5)
  
  test_results = 
    t.test(sample, mu = 0) |> 
    broom::tidy() |> 
    select(estimate, p.value)
  
  test_results
}
```

Run the function for mu from 0 to 6 for 5000 times and record the results:

```{r}
rnorm_sim_results_df = 
  expand_grid(
    mu = 0:6,
    iter = 1:5000
  ) |> 
  mutate(
    results = map(mu, rnorm_sim)
  ) |> 
  unnest(results)
```

calculate power and create `power_df` for plotting:

```{r}
power_df = 
  rnorm_sim_results_df |> 
  group_by(
    mu
  ) |> 
  summarize(
    power = mean(p.value < 0.05)
  )
```

Plot power vs. true mu:

```{r}
power_df |> 
  ggplot(aes(x = mu, y = power)) +
  geom_point() +
  geom_line() +
  labs(
    x = "True mean (Î¼)",
    y = "Power"
  )
```

The plot shows that power rises as the true mean increases, which means that larger effect sizes produce higher power. 
Create `estimate_df` containing `estimate` info 

```{r}
estimate_df = 
  rnorm_sim_results_df |> 
  group_by(mu) |> 
  summarize(
    avg_estimate_all = mean(estimate),
    avg_estimate_rejected = mean(estimate[p.value < 0.05])
  )
```

Plot estimate vs. true mean:

```{r}
estimate_df |> 
  ggplot(aes(x = mu)) +
  geom_point(aes(y = avg_estimate_all, color = "all samples")) +
  geom_point(aes(y = avg_estimate_rejected, color = "rejected samples")) +
  labs(
    x = "True mean (Î¼)",
    y = "Average estimate of Î¼Ì‚",
    title = "Average estimates vs true Î¼",
    color = "Sample"
  )
```

No, it is not equal to the true mu. Because the rejected samples rejected the null when they have very large estimated mu compared to the true mu, so the sample average of ðœ‡Ì‚ across tests for which the null is rejected will be larger than the true mu, especially when the effect size is small. 


## Problem 3

Load the dataset
```{r}
homicide_df = read_csv("homicide-data.csv")
```

There are `r nrow(homicide_df)` rows and `r ncol(homicide_df)` columns. Each row represents a single homicide case reported in one of 50 large U.S. cities. The variables include victim info (`victim_last`, `victim_first`, `victim_age`, `victim_race`, and `victim_sex`), geographic info (`city`, `state`, `lat`, `lon`), the date when the homicide was reported (`reported_date`), and whether the homicide was solved or remains unsolved (`disposition`).
There is one data point: `city` = TULSA, `state` = AL, which may be a point recorded by mistake. In the following data analysis, this data point is removed for accuracy.


create `city_state` variable
```{r}
homicide_df =
  homicide_df|> 
  mutate(
    city_state = str_c(city, ", ", state)
  ) |>
  filter(city_state != "Tulsa, AL") |> 
  group_by(city_state) |> 
  summarize(
    total = n(),
    unsolved = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
  )
```

Baltimore, MD homicide info:

```{r}
baltimore = 
  homicide_df |> 
  filter(city_state == "Baltimore, MD") |> 
  mutate(
  prop_test = map2(unsolved, total, ~ broom::tidy(prop.test(.x, .y)))
  ) |> 
  unnest(prop_test) |> 
  select(city_state, estimate, conf.low, conf.high)

baltimore
```

All cities homicide info:

```{r}
all_city_df = 
  homicide_df |> 
  mutate(
  prop_test = map2(unsolved, total, ~ broom::tidy(prop.test(.x, .y)))
  ) |> 
  unnest(prop_test) |> 
  select(city_state, estimate, conf.low, conf.high)
```

Create plots: 

```{r}
all_city_df |> 
  mutate(
    city_state = fct_reorder(city_state, estimate)
  ) |> 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  labs(
    x = "City",
    y = "Proportion unsolved homicides (95% CI)"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

